{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb7e057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a25e4ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text dataset shape after cleaning: (91965, 2)\n",
      "Text dataset columns: ['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Load Speech Dataset\n",
    "# ----------------------------\n",
    "speech_dir = 'dataset/speech'\n",
    "speech_data = []\n",
    "\n",
    "for file in os.listdir(speech_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        parts = file.split('_')\n",
    "        if len(parts) == 3:\n",
    "            word = parts[1]\n",
    "            emotion = parts[2].replace('.wav', '')\n",
    "            speech_data.append({\n",
    "                'word': word,\n",
    "                'emotion': emotion,\n",
    "                'speech_path': os.path.join(speech_dir, file)\n",
    "            })\n",
    "\n",
    "speech_df = pd.DataFrame(speech_data)\n",
    "speech_df.to_csv('speech_word_dataset.csv', index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load Text Dataset\n",
    "# ----------------------------\n",
    "def load_csvs_from_dir(directory):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "text_train_df = load_csvs_from_dir(\"dataset/text/train\")\n",
    "text_val_df = load_csvs_from_dir(\"dataset/text/validation\")\n",
    "text_test_df = load_csvs_from_dir(\"dataset/text/test\")\n",
    "text_df = pd.concat([text_train_df, text_val_df, text_test_df], ignore_index=True)\n",
    "\n",
    "# Clean text data - remove NaN values and ensure all text entries are strings\n",
    "text_df = text_df.dropna(subset=['text', 'label'])\n",
    "text_df['text'] = text_df['text'].astype(str)\n",
    "print(f\"Text dataset shape after cleaning: {text_df.shape}\")\n",
    "print(f\"Text dataset columns: {text_df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ad776a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 19\n",
      "Classes: ['anger' 'angry' 'boredom' 'disgust' 'empty' 'enthusiasm' 'fear' 'fun'\n",
      " 'happiness' 'happy' 'hate' 'love' 'neutral' 'ps' 'relief' 'sad' 'sadness'\n",
      " 'surprise' 'worry']\n",
      "Speech dataset shape: (1200, 4)\n",
      "Text dataset shape: (91965, 2)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 3. Encode Labels (Shared)\n",
    "# ----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = pd.concat([speech_df['emotion'], text_df['label']], ignore_index=True)\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "speech_df['label'] = label_encoder.transform(speech_df['emotion'])\n",
    "text_df['label'] = label_encoder.transform(text_df['label'])\n",
    "\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "print(f\"Speech dataset shape: {speech_df.shape}\")\n",
    "print(f\"Text dataset shape: {text_df.shape}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Tokenizer and BERT Model\n",
    "# ----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Feature Extraction Utils\n",
    "# ----------------------------\n",
    "def extract_mfcc(wav_path, max_len=100):\n",
    "    y, sr = librosa.load(wav_path, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0,0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fd9cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Separate Datasets for Each Modality\n",
    "# ----------------------------\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, speech_df):\n",
    "        self.speech_df = speech_df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speech_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speech_row = self.speech_df.iloc[idx]\n",
    "        mfcc = extract_mfcc(speech_row['speech_path'])\n",
    "        label = torch.tensor(speech_row['label'], dtype=torch.long)\n",
    "        return torch.tensor(mfcc, dtype=torch.float32), label\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_df):\n",
    "        self.text_df = text_df.reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_row = self.text_df.iloc[idx]\n",
    "        \n",
    "        # Ensure text is a string and handle any potential issues\n",
    "        text = str(text_row['text']).strip()\n",
    "        if not text or text == 'nan':\n",
    "            text = \"empty text\"  # fallback for empty texts\n",
    "            \n",
    "        text_input = tokenizer(text, return_tensors=\"pt\", \n",
    "                              padding=\"max_length\", truncation=True, max_length=32)\n",
    "        text_input = {k: v.squeeze(0) for k, v in text_input.items()}\n",
    "        label = torch.tensor(text_row['label'], dtype=torch.long)\n",
    "        return text_input, label\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Collate Functions for Each Modality\n",
    "# ----------------------------\n",
    "def audio_collate_fn(batch):\n",
    "    mfccs, labels = zip(*batch)\n",
    "    mfccs = nn.utils.rnn.pad_sequence(mfccs, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    return mfccs, labels\n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    text_inputs, labels = zip(*batch)\n",
    "    input_ids = torch.stack([ti['input_ids'] for ti in text_inputs])\n",
    "    attention_mask = torch.stack([ti['attention_mask'] for ti in text_inputs])\n",
    "    text_input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    labels = torch.tensor(labels)\n",
    "    return text_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "307dec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8. Individual Models\n",
    "# ----------------------------\n",
    "class AudioLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim=40, hidden_dim=128, num_layers=2, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),  # *2 for bidirectional\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, features]\n",
    "        lstm_out, (hidden, _) = self.lstm(x)\n",
    "        # Use the last output\n",
    "        last_output = lstm_out[:, -1, :]  # [batch, hidden_dim*2]\n",
    "        output = self.classifier(last_output)\n",
    "        return output\n",
    "\n",
    "class TextBERTModel(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_input):\n",
    "        bert_output = self.bert(**text_input)\n",
    "        text_feat = bert_output.last_hidden_state[:, 0, :]  # CLS token\n",
    "        output = self.classifier(text_feat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14f408cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 9. Training Functions\n",
    "# ----------------------------\n",
    "def train_individual_model(model, train_loader, val_loader, epochs=15, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            if isinstance(inputs, dict):  # Text input\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            else:  # Audio input\n",
    "                inputs = inputs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        train_acc = total_correct / total_samples\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_acc, val_loss = evaluate_individual_model(model, val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc*100:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_individual_model(model, loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            if isinstance(inputs, dict):  # Text input\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            else:  # Audio input\n",
    "                inputs = inputs.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return accuracy, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0b68e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Audio LSTM Model...\n",
      "Epoch 1, Train Loss: 2.9376, Train Acc: 11.25%, Val Loss: 2.9056, Val Acc: 18.33%\n",
      "Epoch 2, Train Loss: 2.6891, Train Acc: 17.40%, Val Loss: 2.3245, Val Acc: 15.83%\n",
      "Epoch 3, Train Loss: 2.2000, Train Acc: 15.21%, Val Loss: 1.9999, Val Acc: 13.75%\n",
      "Epoch 4, Train Loss: 2.0696, Train Acc: 15.62%, Val Loss: 1.9084, Val Acc: 13.75%\n",
      "Epoch 5, Train Loss: 1.9934, Train Acc: 16.04%, Val Loss: 1.8693, Val Acc: 15.83%\n",
      "Epoch 6, Train Loss: 1.9407, Train Acc: 17.71%, Val Loss: 1.8461, Val Acc: 15.83%\n",
      "Epoch 7, Train Loss: 1.9422, Train Acc: 17.50%, Val Loss: 1.8306, Val Acc: 27.50%\n",
      "Epoch 8, Train Loss: 1.9418, Train Acc: 16.56%, Val Loss: 1.8232, Val Acc: 27.50%\n",
      "Epoch 9, Train Loss: 1.8920, Train Acc: 19.69%, Val Loss: 1.7557, Val Acc: 22.50%\n",
      "Epoch 10, Train Loss: 1.7590, Train Acc: 26.35%, Val Loss: 1.6071, Val Acc: 44.58%\n",
      "Epoch 11, Train Loss: 1.6231, Train Acc: 33.75%, Val Loss: 1.4366, Val Acc: 47.08%\n",
      "Epoch 12, Train Loss: 1.4532, Train Acc: 37.40%, Val Loss: 1.2807, Val Acc: 49.58%\n",
      "Epoch 13, Train Loss: 1.2987, Train Acc: 43.02%, Val Loss: 1.1557, Val Acc: 47.08%\n",
      "Epoch 14, Train Loss: 1.1675, Train Acc: 47.92%, Val Loss: 1.0328, Val Acc: 45.83%\n",
      "Epoch 15, Train Loss: 1.0704, Train Acc: 49.69%, Val Loss: 0.9525, Val Acc: 46.25%\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 10. Train Audio LSTM Model\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Create audio dataset and data loaders\n",
    "audio_dataset = AudioDataset(speech_df)\n",
    "audio_train_indices, audio_val_indices = train_test_split(\n",
    "    list(range(len(audio_dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "audio_train_loader = DataLoader(\n",
    "    torch.utils.data.Subset(audio_dataset, audio_train_indices),\n",
    "    batch_size=16, shuffle=True, collate_fn=audio_collate_fn)\n",
    "audio_val_loader = DataLoader(\n",
    "    torch.utils.data.Subset(audio_dataset, audio_val_indices),\n",
    "    batch_size=16, shuffle=False, collate_fn=audio_collate_fn)\n",
    "\n",
    "# Train Audio LSTM Model\n",
    "print(\"Training Audio LSTM Model...\")\n",
    "audio_model = AudioLSTMModel(num_classes=num_classes)\n",
    "audio_model = train_individual_model(audio_model, audio_train_loader, audio_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36bb0767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Text BERT Model...\n",
      "Epoch 1, Train Loss: 2.0366, Train Acc: 29.16%, Val Loss: 2.1417, Val Acc: 21.32%\n",
      "Epoch 2, Train Loss: 2.1576, Train Acc: 21.31%, Val Loss: 2.1391, Val Acc: 21.32%\n",
      "Epoch 3, Train Loss: 2.1537, Train Acc: 21.51%, Val Loss: 2.1434, Val Acc: 21.32%\n",
      "Epoch 4, Train Loss: 2.1523, Train Acc: 21.48%, Val Loss: 2.1435, Val Acc: 21.32%\n",
      "Epoch 5, Train Loss: 2.1512, Train Acc: 21.50%, Val Loss: 2.1388, Val Acc: 21.32%\n",
      "Epoch 6, Train Loss: 2.1516, Train Acc: 21.43%, Val Loss: 2.1397, Val Acc: 21.32%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Text BERT Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m text_model \u001b[38;5;241m=\u001b[39m TextBERTModel(bert_model, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m---> 19\u001b[0m text_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_individual_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_val_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 32\u001b[0m, in \u001b[0;36mtrain_individual_model\u001b[1;34m(model, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 32\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 11. Train Text BERT Model\n",
    "# ----------------------------\n",
    "# Create text dataset and data loaders\n",
    "text_dataset = TextDataset(text_df)\n",
    "text_train_indices, text_val_indices = train_test_split(\n",
    "    list(range(len(text_dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "text_train_loader = DataLoader(\n",
    "    torch.utils.data.Subset(text_dataset, text_train_indices),\n",
    "    batch_size=16, shuffle=True, collate_fn=text_collate_fn)\n",
    "text_val_loader = DataLoader(\n",
    "    torch.utils.data.Subset(text_dataset, text_val_indices),\n",
    "    batch_size=16, shuffle=False, collate_fn=text_collate_fn)\n",
    "\n",
    "# Train Text BERT Model\n",
    "print(\"\\nTraining Text BERT Model...\")\n",
    "text_model = TextBERTModel(bert_model, num_classes=num_classes)\n",
    "text_model = train_individual_model(text_model, text_train_loader, text_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fdf284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 12. Late Fusion Model\n",
    "# ----------------------------\n",
    "class LateFusionModel(nn.Module):\n",
    "    def __init__(self, audio_model, text_model, num_classes, fusion_method='average'):\n",
    "        super().__init__()\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.fusion_method = fusion_method\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        if fusion_method == 'weighted':\n",
    "            self.audio_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.text_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        elif fusion_method == 'learned':\n",
    "            # Learn to combine the logits - dynamically size based on actual number of classes\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(num_classes * 2, 64),  # num_classes * 2 modalities\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, audio_input, text_input):\n",
    "        audio_logits = self.audio_model(audio_input)\n",
    "        text_logits = self.text_model(text_input)\n",
    "        \n",
    "        if self.fusion_method == 'average':\n",
    "            # Simple average of predictions\n",
    "            fused_logits = (audio_logits + text_logits) / 2\n",
    "        elif self.fusion_method == 'weighted':\n",
    "            # Learnable weighted average\n",
    "            weights = F.softmax(torch.stack([self.audio_weight, self.text_weight]), dim=0)\n",
    "            fused_logits = weights[0] * audio_logits + weights[1] * text_logits\n",
    "        elif self.fusion_method == 'learned':\n",
    "            # Learn to combine concatenated logits\n",
    "            combined = torch.cat([audio_logits, text_logits], dim=1)\n",
    "            fused_logits = self.fusion_layer(combined)\n",
    "        \n",
    "        return fused_logits, audio_logits, text_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 13. Combined Dataset for Late Fusion Evaluation\n",
    "# ----------------------------\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, speech_df, text_df):\n",
    "        self.speech_df = speech_df.reset_index(drop=True)\n",
    "        self.text_df = text_df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.speech_df), len(self.text_df))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speech_row = self.speech_df.iloc[idx]\n",
    "        text_row = self.text_df.iloc[idx]\n",
    "\n",
    "        mfcc = extract_mfcc(speech_row['speech_path'])\n",
    "        label = torch.tensor(speech_row['label'], dtype=torch.long)\n",
    "\n",
    "        text_input = tokenizer(text_row['text'], return_tensors=\"pt\", \n",
    "                              padding=\"max_length\", truncation=True, max_length=32)\n",
    "        text_input = {k: v.squeeze(0) for k, v in text_input.items()}\n",
    "\n",
    "        return torch.tensor(mfcc, dtype=torch.float32), text_input, label\n",
    "\n",
    "def combined_collate_fn(batch):\n",
    "    mfccs, text_inputs, labels = zip(*batch)\n",
    "    \n",
    "    # Pad MFCC sequences\n",
    "    mfccs = nn.utils.rnn.pad_sequence(mfccs, batch_first=True)\n",
    "    \n",
    "    # Stack BERT inputs\n",
    "    input_ids = torch.stack([ti['input_ids'] for ti in text_inputs])\n",
    "    attention_mask = torch.stack([ti['attention_mask'] for ti in text_inputs])\n",
    "    text_input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    \n",
    "    labels = torch.tensor(labels)\n",
    "    return mfccs, text_input, labels\n",
    "\n",
    "# Create combined dataset for evaluation\n",
    "combined_dataset = CombinedDataset(speech_df, text_df)\n",
    "combined_train_indices, combined_val_indices = train_test_split(\n",
    "    list(range(len(combined_dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "combined_val_loader = DataLoader(\n",
    "    torch.utils.data.Subset(combined_dataset, combined_val_indices),\n",
    "    batch_size=16, shuffle=False, collate_fn=combined_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Late Fusion with average method...\n",
      "Audio Model Accuracy: 61.67%\n",
      "Text Model Accuracy: 0.00%\n",
      "Late Fusion (average) Accuracy: 0.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating Late Fusion with weighted method...\n",
      "Audio Model Accuracy: 61.67%\n",
      "Text Model Accuracy: 0.00%\n",
      "Late Fusion (weighted) Accuracy: 0.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating Late Fusion with learned method...\n",
      "Training fusion layer...\n",
      "Fusion training epoch 1, Loss: 2.9730\n",
      "Fusion training epoch 2, Loss: 1.6419\n",
      "Fusion training epoch 3, Loss: 1.4100\n",
      "Fusion training epoch 4, Loss: 1.3153\n",
      "Fusion training epoch 5, Loss: 1.2096\n",
      "Audio Model Accuracy: 61.67%\n",
      "Text Model Accuracy: 0.00%\n",
      "Late Fusion (learned) Accuracy: 58.75%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 14. Evaluate Late Fusion Methods\n",
    "# ----------------------------\n",
    "fusion_methods = ['average', 'weighted', 'learned']\n",
    "\n",
    "for method in fusion_methods:\n",
    "    print(f\"\\nEvaluating Late Fusion with {method} method...\")\n",
    "    \n",
    "    # Create late fusion model\n",
    "    late_fusion_model = LateFusionModel(audio_model, text_model, fusion_method=method, num_classes=num_classes)\n",
    "    late_fusion_model = late_fusion_model.to(device)\n",
    "    \n",
    "    # If using learned fusion, train the fusion layer\n",
    "    if method == 'learned':\n",
    "        optimizer = torch.optim.Adam(late_fusion_model.fusion_layer.parameters(), lr=1e-3)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train fusion layer for a few epochs\n",
    "        print(\"Training fusion layer...\")\n",
    "        for epoch in range(5):\n",
    "            late_fusion_model.train()\n",
    "            total_loss = 0\n",
    "            for mfccs, text_inputs, labels in combined_val_loader:\n",
    "                mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "                text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                fused_logits, _, _ = late_fusion_model(mfccs, text_inputs)\n",
    "                loss = criterion(fused_logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f\"Fusion training epoch {epoch+1}, Loss: {total_loss/len(combined_val_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate late fusion model\n",
    "    late_fusion_model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    audio_correct = 0\n",
    "    text_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for mfccs, text_inputs, labels in combined_val_loader:\n",
    "            mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "            text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "            \n",
    "            fused_logits, audio_logits, text_logits = late_fusion_model(mfccs, text_inputs)\n",
    "            \n",
    "            # Predictions\n",
    "            fused_preds = torch.argmax(fused_logits, dim=1)\n",
    "            audio_preds = torch.argmax(audio_logits, dim=1)\n",
    "            text_preds = torch.argmax(text_logits, dim=1)\n",
    "            \n",
    "            # Accuracies\n",
    "            total_correct += (fused_preds == labels).sum().item()\n",
    "            audio_correct += (audio_preds == labels).sum().item()\n",
    "            text_correct += (text_preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    fusion_acc = total_correct / total_samples\n",
    "    audio_acc = audio_correct / total_samples\n",
    "    text_acc = text_correct / total_samples\n",
    "    \n",
    "    print(f\"Audio Model Accuracy: {audio_acc*100:.2f}%\")\n",
    "    print(f\"Text Model Accuracy: {text_acc*100:.2f}%\")\n",
    "    print(f\"Late Fusion ({method}) Accuracy: {fusion_acc*100:.2f}%\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
