{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eef3aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4963e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, n_mfcc=40, max_len=100):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc = mfcc.T  # Shape: (time, n_mfcc)\n",
    "\n",
    "    if mfcc.shape[0] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[0]\n",
    "        mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:max_len, :]\n",
    "    return mfcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64940ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, text_paths, speech_dir, tokenizer, label2id, max_text_len=64, max_audio_len=100, n_mfcc=40):\n",
    "        # Load text data as before\n",
    "        self.samples = []\n",
    "        for path in text_paths:\n",
    "            df = pd.read_csv(path)\n",
    "            for _, row in df.iterrows():\n",
    "                text = str(row['text'])\n",
    "                label = str(row['label'])\n",
    "                # Assume speech file name is derived from text row or label\n",
    "                speech_file = f\"{speech_dir}/{row['speech_file']}\"  # You need to add this column or logic\n",
    "                self.samples.append((text, speech_file, label2id[label]))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_text_len = max_text_len\n",
    "        self.max_audio_len = max_audio_len\n",
    "        self.n_mfcc = n_mfcc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, speech_file, label = self.samples[idx]\n",
    "\n",
    "        # Text encoding\n",
    "        encoded = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_text_len, return_tensors='pt')\n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "\n",
    "        # MFCC extraction\n",
    "        mfcc = extract_mfcc(speech_file, n_mfcc=self.n_mfcc, max_len=self.max_audio_len)\n",
    "        mfcc = torch.tensor(mfcc, dtype=torch.float)\n",
    "\n",
    "        return input_ids, attention_mask, mfcc, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a7cd609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, text_hidden, audio_feat_dim, audio_hidden, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        # Text branch\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.text_lstm = nn.LSTM(embed_dim, text_hidden, batch_first=True)\n",
    "        # Audio branch\n",
    "        self.audio_lstm = nn.LSTM(audio_feat_dim, audio_hidden, batch_first=True)\n",
    "        # Fusion\n",
    "        self.fc = nn.Linear(text_hidden + audio_hidden, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, mfcc):\n",
    "        # Text\n",
    "        embedded = self.embedding(input_ids)\n",
    "        lengths = attention_mask.sum(dim=1).cpu()\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (text_hidden, _) = self.text_lstm(packed)\n",
    "        # Audio\n",
    "        _, (audio_hidden, _) = self.audio_lstm(mfcc)\n",
    "        # Concatenate last hidden states\n",
    "        fused = torch.cat([text_hidden[-1], audio_hidden[-1]], dim=1)\n",
    "        return self.fc(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bebe6ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'speech_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'speech_file'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m label2id \u001b[38;5;241m=\u001b[39m {label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels)}\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Datasets\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMultimodalDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeech_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m MultimodalDataset(val_paths, speech_dir, tokenizer, label2id)\n\u001b[0;32m     20\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m MultimodalDataset(test_paths, speech_dir, tokenizer, label2id)\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mMultimodalDataset.__init__\u001b[1;34m(self, text_paths, speech_dir, tokenizer, label2id, max_text_len, max_audio_len, n_mfcc)\u001b[0m\n\u001b[0;32m      9\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# Assume speech file name is derived from text row or label\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m         speech_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeech_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeech_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# You need to add this column or logic\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;241m.\u001b[39mappend((text, speech_file, label2id[label]))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'speech_file'"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "\n",
    "# Paths\n",
    "speech_dir = \"dataset/speech\"\n",
    "train_paths = [\"dataset/text/train/training.csv\", \"dataset/text/train/training2.csv\", \"dataset/text/train/training3.csv\"]\n",
    "val_paths = [\"dataset/text/validation/validation.csv\",\"dataset/text/validation/validation2.csv\",\"dataset/text/validation/validation3.csv\"]\n",
    "test_paths = [\"dataset/text/test/test.csv\",\"dataset/text/test/test2.csv\",\"dataset/text/test/test3.csv\",\"dataset/text/test/testing3.csv\"]\n",
    "\n",
    "# Label mapping\n",
    "df_train = pd.concat([pd.read_csv(p) for p in train_paths])\n",
    "df_train['label'] = df_train['label'].astype(str)\n",
    "labels = sorted(df_train['label'].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# Datasets\n",
    "train_dataset = MultimodalDataset(train_paths, speech_dir, tokenizer, label2id)\n",
    "val_dataset = MultimodalDataset(val_paths, speech_dir, tokenizer, label2id)\n",
    "test_dataset = MultimodalDataset(test_paths, speech_dir, tokenizer, label2id)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Model\n",
    "model = MultimodalLSTM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=128,\n",
    "    output_dim=len(label2id),\n",
    "    pad_idx=pad_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b71f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.5430\n",
      "Validation Accuracy: 0.3631\n",
      "Epoch 2 | Loss: 0.3529\n",
      "Validation Accuracy: 0.3516\n",
      "Epoch 3 | Loss: 0.2334\n",
      "Validation Accuracy: 0.3534\n",
      "Epoch 4 | Loss: 0.1591\n",
      "Validation Accuracy: 0.3440\n",
      "Epoch 5 | Loss: 0.1170\n",
      "Validation Accuracy: 0.3392\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_ids, attention_mask, labels in train_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, mfcc)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in val_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask, mfcc)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = correct / total if total > 0 else 0\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
