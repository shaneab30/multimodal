{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4baeeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column: word in text dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m required_cols:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m text_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in text dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Lowercase for merge\u001b[39;00m\n\u001b[1;32m     40\u001b[0m text_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m text_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[0;31mValueError\u001b[0m: Missing column: word in text dataset"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Load Speech Dataset\n",
    "# ----------------------------\n",
    "speech_dir = 'dataset/speech'\n",
    "speech_data = []\n",
    "\n",
    "for file in os.listdir(speech_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        parts = file.split('_')\n",
    "        if len(parts) == 3:\n",
    "            word = parts[1]\n",
    "            emotion = parts[2].replace('.wav', '')\n",
    "            speech_data.append({\n",
    "                'word': word,\n",
    "                'emotion': emotion,\n",
    "                'speech_path': os.path.join(speech_dir, file)\n",
    "            })\n",
    "\n",
    "speech_df = pd.DataFrame(speech_data)\n",
    "speech_df.to_csv('speech_word_dataset.csv', index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load Text Dataset\n",
    "# ----------------------------\n",
    "def load_csvs_from_dir(directory):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "text_train_df = load_csvs_from_dir(\"dataset/text/train\")\n",
    "text_val_df = load_csvs_from_dir(\"dataset/text/validation\")\n",
    "text_test_df = load_csvs_from_dir(\"dataset/text/test\")\n",
    "text_df = pd.concat([text_train_df, text_val_df, text_test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ea854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Encode Labels (Shared)\n",
    "# ----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = pd.concat([speech_df['emotion'], text_df['label']], ignore_index=True)\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "speech_df['label'] = label_encoder.transform(speech_df['emotion'])\n",
    "text_df['label'] = label_encoder.transform(text_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Tokenizer and BERT Model\n",
    "# ----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Feature Extraction Utils\n",
    "# ----------------------------\n",
    "def extract_mfcc(wav_path, max_len=100):\n",
    "    y, sr = librosa.load(wav_path, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0,0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T\n",
    "def extract_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Early Fusion Dataset\n",
    "# ----------------------------\n",
    "class EarlyFusionDataset(Dataset):\n",
    "    def __init__(self, speech_df, text_df):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Matching based on emotion class\n",
    "        min_samples = min(len(speech_df), len(text_df))\n",
    "        for i in range(min_samples):\n",
    "            speech_row = speech_df.iloc[i]\n",
    "            text_row = text_df.iloc[i]\n",
    "\n",
    "            # Extract features\n",
    "            mfcc = extract_mfcc(speech_row['speech_path'])  # shape: [time, 40]\n",
    "            bert = extract_bert_embedding(text_row['text'])  # shape: [768]\n",
    "\n",
    "            # Concatenate\n",
    "            bert_repeated = np.repeat(bert[np.newaxis, :], mfcc.shape[0], axis=0)  # [time, 768]\n",
    "            fused = np.concatenate((mfcc, bert_repeated), axis=1)  # [time, 808]\n",
    "\n",
    "            self.features.append(torch.tensor(fused, dtype=torch.float32))\n",
    "            self.labels.append(torch.tensor(speech_row['label'], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7. Collate Function for Padding\n",
    "# ----------------------------\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    padded = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    return padded, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8. LSTM Model\n",
    "# ----------------------------\n",
    "class EarlyFusionLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=808, hidden_dim=128, num_classes=6):\n",
    "        super(EarlyFusionLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 9. Training\n",
    "# ----------------------------\n",
    "dataset = EarlyFusionDataset(speech_df, text_df)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EarlyFusionLSTM(num_classes=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
