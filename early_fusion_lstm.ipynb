{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4baeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1. Data Loading Functions\n",
    "# -----------------------\n",
    "\n",
    "def load_text_data(text_dir):\n",
    "    \"\"\"Load text data from train/test/validation folders\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        split_path = os.path.join(text_dir, split)\n",
    "        if os.path.exists(split_path):\n",
    "            for file in os.listdir(split_path):\n",
    "                if file.endswith('.csv'):\n",
    "                    file_path = os.path.join(split_path, file)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    df['split'] = split\n",
    "                    all_data.append(df)\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No text data found!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_speech_data(audio_dir):\n",
    "    \"\"\"Load speech data by scanning audio files\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for file in os.listdir(audio_dir):\n",
    "        if file.endswith('.wav'):\n",
    "            parts = file.split('_')\n",
    "            if len(parts) == 3:\n",
    "                word = parts[1]\n",
    "                emotion = parts[2].replace('.wav', '')\n",
    "                data.append({\n",
    "                    'word': word,\n",
    "                    'emotion': emotion,\n",
    "                    'speech_path': os.path.join(audio_dir, file)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_dataset(text_df, speech_df):\n",
    "    \"\"\"Combine text and speech datasets by matching words and emotions\"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    # Check what columns exist in text_df\n",
    "    print(f\"Text DataFrame columns: {text_df.columns.tolist()}\")\n",
    "    print(f\"Text DataFrame shape: {text_df.shape}\")\n",
    "    print(\"Sample text data:\")\n",
    "    print(text_df.head())\n",
    "    \n",
    "    # Get unique combinations from both datasets\n",
    "    if 'word' in text_df.columns and 'emotion' in text_df.columns:\n",
    "        text_combinations = set(zip(text_df['word'], text_df['emotion']))\n",
    "    else:\n",
    "        # Assume first column is word, second is emotion\n",
    "        text_combinations = set(zip(text_df.iloc[:, 0], text_df.iloc[:, 1]))\n",
    "    \n",
    "    speech_combinations = set(zip(speech_df['word'], speech_df['emotion']))\n",
    "    \n",
    "    print(f\"Text combinations found: {len(text_combinations)}\")\n",
    "    print(f\"Speech combinations found: {len(speech_combinations)}\")\n",
    "    \n",
    "    # Find common combinations\n",
    "    common_combinations = text_combinations.intersection(speech_combinations)\n",
    "    print(f\"Common combinations: {len(common_combinations)}\")\n",
    "    \n",
    "    for word, emotion in common_combinations:\n",
    "        # Get text data\n",
    "        if 'word' in text_df.columns and 'emotion' in text_df.columns:\n",
    "            text_row = text_df[(text_df['word'] == word) & (text_df['emotion'] == emotion)]\n",
    "        else:\n",
    "            text_row = text_df[(text_df.iloc[:, 0] == word) & (text_df.iloc[:, 1] == emotion)]\n",
    "        \n",
    "        # Get speech data\n",
    "        speech_row = speech_df[(speech_df['word'] == word) & (speech_df['emotion'] == emotion)]\n",
    "        \n",
    "        if not text_row.empty and not speech_row.empty:\n",
    "            # Get text content (check for various column names)\n",
    "            text_content = word  # fallback to word\n",
    "            if 'text' in text_row.columns:\n",
    "                text_content = text_row['text'].iloc[0]\n",
    "            elif 'sentence' in text_row.columns:\n",
    "                text_content = text_row['sentence'].iloc[0]\n",
    "            elif len(text_row.columns) > 2:\n",
    "                text_content = text_row.iloc[0, 2]\n",
    "            \n",
    "            combined_data.append({\n",
    "                'word': word,\n",
    "                'emotion': emotion,\n",
    "                'text_content': str(text_content),\n",
    "                'speech_path': speech_row['speech_path'].iloc[0]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2. Preprocessing Functions\n",
    "# -----------------------\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=40, max_len=100):\n",
    "    \"\"\"Extract MFCC features from audio\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        delta = librosa.feature.delta(mfcc)\n",
    "        delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        combined = np.vstack([mfcc, delta, delta2]).T  # (time, feature_dim)\n",
    "\n",
    "        if combined.shape[0] < max_len:\n",
    "            pad_width = max_len - combined.shape[0]\n",
    "            combined = np.pad(combined, ((0, pad_width), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            combined = combined[:max_len, :]\n",
    "        \n",
    "        # Normalize\n",
    "        if np.std(combined) > 0:\n",
    "            combined = (combined - np.mean(combined)) / np.std(combined)\n",
    "        \n",
    "        return combined\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        # Return zero array if file can't be processed\n",
    "        return np.zeros((max_len, n_mfcc * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64940ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3. Dataset Class\n",
    "# -----------------------\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, label_encoder, max_text_length=128, max_audio_length=100):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_text_length = max_text_length\n",
    "        self.max_audio_length = max_audio_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Process text\n",
    "        text_content = str(row['text_content'])\n",
    "        text_encoding = self.tokenizer(\n",
    "            text_content,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_text_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Process audio\n",
    "        mfcc = extract_mfcc(row['speech_path'], max_len=self.max_audio_length)\n",
    "        \n",
    "        # Process label\n",
    "        label = self.label_encoder.transform([row['emotion']])[0]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(),\n",
    "            'mfcc': torch.tensor(mfcc, dtype=torch.float32),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4. Model Architecture\n",
    "# -----------------------\n",
    "\n",
    "class MultimodalEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, text_model_name='bert-base-uncased', \n",
    "                 lstm_hidden=128, audio_feature_dim=120):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder (BERT)\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_hidden_size = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Audio encoder (CNN + LSTM)\n",
    "        self.audio_cnn = nn.Sequential(\n",
    "            nn.Conv1d(audio_feature_dim, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        \n",
    "        self.audio_lstm = nn.LSTM(64, lstm_hidden, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fusion layer\n",
    "        fusion_input_dim = text_hidden_size + (lstm_hidden * 2)  # *2 for bidirectional\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, mfcc):\n",
    "        # Text encoding\n",
    "        text_outputs = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        text_features = text_outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Audio encoding\n",
    "        # mfcc: [batch_size, time, features] -> [batch_size, features, time]\n",
    "        audio_cnn_input = mfcc.permute(0, 2, 1)\n",
    "        audio_cnn_out = self.audio_cnn(audio_cnn_input)  # [batch_size, 64, time]\n",
    "        \n",
    "        # Back to [batch_size, time, features] for LSTM\n",
    "        audio_lstm_input = audio_cnn_out.permute(0, 2, 1)\n",
    "        _, (h_n, _) = self.audio_lstm(audio_lstm_input)\n",
    "        \n",
    "        # Concatenate forward and backward hidden states\n",
    "        audio_features = torch.cat((h_n[-2], h_n[-1]), dim=1)  # [batch_size, lstm_hidden*2]\n",
    "        \n",
    "        # Fusion\n",
    "        combined_features = torch.cat((text_features, audio_features), dim=1)\n",
    "        fusion_output = self.fusion_layer(combined_features)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fusion_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd90cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 5. Training Functions\n",
    "# -----------------------\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        mfcc = batch['mfcc'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, mfcc)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            mfcc = batch['mfcc'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, mfcc)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 6. Load and Prepare Data\n",
    "# -----------------------\n",
    "\n",
    "# Set paths\n",
    "text_dir = 'dataset/text'\n",
    "audio_dir = 'dataset/speech'\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading text dataset...\")\n",
    "text_df = load_text_data(text_dir)\n",
    "print(f\"Text dataset shape: {text_df.shape}\")\n",
    "\n",
    "print(\"\\nLoading speech dataset...\")\n",
    "speech_df = load_speech_data(audio_dir)\n",
    "print(f\"Speech dataset shape: {speech_df.shape}\")\n",
    "\n",
    "# Combine datasets\n",
    "print(\"\\nCombining datasets...\")\n",
    "combined_df = create_combined_dataset(text_df, speech_df)\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "if not combined_df.empty:\n",
    "    print(\"\\nSample combined data:\")\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print(\"ERROR: No matching data found between text and speech datasets!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 7. Prepare for Training (run this cell after checking the data above)\n",
    "# -----------------------\n",
    "\n",
    "# Prepare labels\n",
    "label_encoder = LabelEncoder()\n",
    "combined_df['emotion_encoded'] = label_encoder.fit_transform(combined_df['emotion'])\n",
    "\n",
    "print(f\"Emotion classes: {label_encoder.classes_}\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(combined_df, test_size=0.2, random_state=42, \n",
    "                                   stratify=combined_df['emotion_encoded'])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MultimodalDataset(train_df, tokenizer, label_encoder)\n",
    "val_dataset = MultimodalDataset(val_df, tokenizer, label_encoder)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "print(\"Datasets and dataloaders created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ddf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 8. Initialize Model and Training\n",
    "# -----------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MultimodalEmotionModel(\n",
    "    num_classes=len(label_encoder.classes_),\n",
    "    text_model_name='bert-base-uncased'\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f977617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 9. Training Loop\n",
    "# -----------------------\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "print(f\"Training completed! Best validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
