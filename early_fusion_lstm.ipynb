{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef3aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4baeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Load Speech Dataset\n",
    "# ----------------------------\n",
    "speech_dir = 'dataset/speech'\n",
    "speech_data = []\n",
    "\n",
    "for file in os.listdir(speech_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        parts = file.split('_')\n",
    "        if len(parts) == 3:\n",
    "            word = parts[1]\n",
    "            emotion = parts[2].replace('.wav', '')\n",
    "            speech_data.append({\n",
    "                'word': word,\n",
    "                'emotion': emotion,\n",
    "                'speech_path': os.path.join(speech_dir, file)\n",
    "            })\n",
    "\n",
    "speech_df = pd.DataFrame(speech_data)\n",
    "speech_df.to_csv('speech_word_dataset.csv', index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load Text Dataset\n",
    "# ----------------------------\n",
    "def load_csvs_from_dir(directory):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "text_train_df = load_csvs_from_dir(\"dataset/text/train\")\n",
    "text_val_df = load_csvs_from_dir(\"dataset/text/validation\")\n",
    "text_test_df = load_csvs_from_dir(\"dataset/text/test\")\n",
    "text_df = pd.concat([text_train_df, text_val_df, text_test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c28ea854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Encode Labels (Shared)\n",
    "# ----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = pd.concat([speech_df['emotion'], text_df['label']], ignore_index=True)\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "speech_df['label'] = label_encoder.transform(speech_df['emotion'])\n",
    "text_df['label'] = label_encoder.transform(text_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e37fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/homebrew/Caskroom/miniconda/base/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4. Tokenizer and BERT Model\n",
    "# ----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Feature Extraction Utils\n",
    "# ----------------------------\n",
    "def extract_mfcc(wav_path, max_len=100):\n",
    "    y, sr = librosa.load(wav_path, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0,0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T\n",
    "def extract_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Early Fusion Dataset\n",
    "# ----------------------------\n",
    "class EarlyFusionDataset(Dataset):\n",
    "    def __init__(self, speech_df, text_df):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Matching based on emotion class\n",
    "        min_samples = min(len(speech_df), len(text_df))\n",
    "        for i in range(min_samples):\n",
    "            speech_row = speech_df.iloc[i]\n",
    "            text_row = text_df.iloc[i]\n",
    "\n",
    "            # Extract features\n",
    "            mfcc = extract_mfcc(speech_row['speech_path'])  # shape: [time, 40]\n",
    "            bert = extract_bert_embedding(text_row['text'])  # shape: [768]\n",
    "\n",
    "            # Concatenate\n",
    "            bert_repeated = np.repeat(bert[np.newaxis, :], mfcc.shape[0], axis=0)  # [time, 768]\n",
    "            fused = np.concatenate((mfcc, bert_repeated), axis=1)  # [time, 808]\n",
    "\n",
    "            self.features.append(torch.tensor(fused, dtype=torch.float32))\n",
    "            self.labels.append(torch.tensor(speech_row['label'], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7. Collate Function for Padding\n",
    "# ----------------------------\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    padded = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    return padded, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8. LSTM Model\n",
    "# ----------------------------\n",
    "class EarlyFusionLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=808, hidden_dim=128, num_classes=6):\n",
    "        super(EarlyFusionLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Training and Evaluation\n",
    "# ----------------------------\n",
    "\n",
    "# Split dataset into train and validation\n",
    "full_dataset = EarlyFusionDataset(speech_df, text_df)\n",
    "train_indices, val_indices = train_test_split(list(range(len(full_dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EarlyFusionLSTM(num_classes=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == batch_y).sum().item()\n",
    "            total_samples += batch_y.size(0)\n",
    "    accuracy = total_correct / total_samples\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Training loop with metrics\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        total_correct += (preds == batch_y).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "\n",
    "    train_acc = total_correct / total_samples\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    val_acc, val_loss = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_acc*100:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
