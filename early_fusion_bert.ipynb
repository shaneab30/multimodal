{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4baeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Load Speech Dataset\n",
    "# ----------------------------\n",
    "speech_dir = 'dataset/speech'\n",
    "speech_data = []\n",
    "\n",
    "for file in os.listdir(speech_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        parts = file.split('_')\n",
    "        if len(parts) == 3:\n",
    "            word = parts[1]\n",
    "            emotion = parts[2].replace('.wav', '')\n",
    "            speech_data.append({\n",
    "                'word': word,\n",
    "                'emotion': emotion,\n",
    "                'speech_path': os.path.join(speech_dir, file)\n",
    "            })\n",
    "\n",
    "speech_df = pd.DataFrame(speech_data)\n",
    "speech_df.to_csv('speech_word_dataset.csv', index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load Text Dataset\n",
    "# ----------------------------\n",
    "def load_csvs_from_dir(directory):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "text_train_df = load_csvs_from_dir(\"dataset/text/train\")\n",
    "text_val_df = load_csvs_from_dir(\"dataset/text/validation\")\n",
    "text_test_df = load_csvs_from_dir(\"dataset/text/test\")\n",
    "text_df = pd.concat([text_train_df, text_val_df, text_test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Encode Labels (Shared)\n",
    "# ----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = pd.concat([speech_df['emotion'], text_df['label']], ignore_index=True)\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "speech_df['label'] = label_encoder.transform(speech_df['emotion'])\n",
    "text_df['label'] = label_encoder.transform(text_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Tokenizer and BERT Model\n",
    "# ----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefacf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Feature Extraction Utils\n",
    "# ----------------------------\n",
    "def extract_mfcc(wav_path, max_len=100):\n",
    "    y, sr = librosa.load(wav_path, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0,0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T\n",
    "def extract_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc938e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Early Fusion Dataset\n",
    "# ----------------------------\n",
    "class EarlyFusionDataset(Dataset):\n",
    "    def __init__(self, speech_df, text_df):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Matching based on emotion class\n",
    "        min_samples = min(len(speech_df), len(text_df))\n",
    "        for i in range(min_samples):\n",
    "            speech_row = speech_df.iloc[i]\n",
    "            text_row = text_df.iloc[i]\n",
    "\n",
    "            # Extract features\n",
    "            mfcc = extract_mfcc(speech_row['speech_path'])  # shape: [time, 40]\n",
    "            bert = extract_bert_embedding(text_row['text'])  # shape: [768]\n",
    "\n",
    "            # Concatenate\n",
    "            bert_repeated = np.repeat(bert[np.newaxis, :], mfcc.shape[0], axis=0)  # [time, 768]\n",
    "            fused = np.concatenate((mfcc, bert_repeated), axis=1)  # [time, 808]\n",
    "\n",
    "            self.features.append(torch.tensor(fused, dtype=torch.float32))\n",
    "            self.labels.append(torch.tensor(speech_row['label'], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speech_row = self.speech_df.iloc[idx]\n",
    "        text_row = self.text_df.iloc[idx]\n",
    "\n",
    "        mfcc = extract_mfcc(speech_row['speech_path'])  # [time, 40]\n",
    "        label = torch.tensor(speech_row['label'], dtype=torch.long)\n",
    "\n",
    "        # Tokenize text for BERT\n",
    "        text_input = tokenizer(text_row['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        text_input = {k: v.squeeze(0) for k, v in text_input.items()}  # remove batch dim\n",
    "\n",
    "        return torch.tensor(mfcc, dtype=torch.float32), text_input, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7. Collate Function for Padding\n",
    "# ----------------------------\n",
    "def collate_fn(batch):\n",
    "    mfccs, text_inputs, labels = zip(*batch)\n",
    "\n",
    "    # Pad MFCC sequences\n",
    "    mfccs = nn.utils.rnn.pad_sequence(mfccs, batch_first=True)\n",
    "\n",
    "    # Stack BERT inputs\n",
    "    input_ids = torch.stack([ti['input_ids'] for ti in text_inputs])\n",
    "    attention_mask = torch.stack([ti['attention_mask'] for ti in text_inputs])\n",
    "    text_input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "    return mfccs, text_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf085793",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBERTAudioFusionModel\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, bert_model, mfcc_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, audio_pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 8. BERT Model\n",
    "# ----------------------------\n",
    "class BERTFusionModel(nn.Module):\n",
    "    def __init__(self, bert_model, mfcc_dim=40, audio_pooling='avg', num_classes=6):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.audio_pooling = audio_pooling\n",
    "        \n",
    "        # If using average pooling for MFCC\n",
    "        self.audio_proj = nn.Linear(mfcc_dim, 128)  # compress MFCC features\n",
    "        \n",
    "        # Fusion Layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768 + 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, mfcc, text_input):\n",
    "        # Get BERT features\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert(**text_input)\n",
    "            text_feat = bert_output.last_hidden_state[:, 0, :]  # CLS token: [batch, 768]\n",
    "\n",
    "        # Aggregate MFCC (avg over time dimension)\n",
    "        audio_feat = mfcc.mean(dim=1)  # [batch, 40]\n",
    "        audio_feat = self.audio_proj(audio_feat)  # [batch, 128]\n",
    "\n",
    "        # Combine\n",
    "        fused = torch.cat((text_feat, audio_feat), dim=1)  # [batch, 896]\n",
    "        out = self.fusion(fused)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 9. Training and Evaluation\n",
    "# ----------------------------\n",
    "\n",
    "# Split dataset into train and validation\n",
    "full_dataset = EarlyFusionDataset(speech_df, text_df)\n",
    "train_indices, val_indices = train_test_split(list(range(len(full_dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTFusionModel(num_classes=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == batch_y).sum().item()\n",
    "            total_samples += batch_y.size(0)\n",
    "    accuracy = total_correct / total_samples\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Training loop with metrics\n",
    "for epoch in range(25):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        total_correct += (preds == batch_y).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "\n",
    "    train_acc = total_correct / total_samples\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    val_acc, val_loss = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_acc*100:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
