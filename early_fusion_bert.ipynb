{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eef3aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc4baeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Load Speech Dataset\n",
    "# ----------------------------\n",
    "speech_dir = 'dataset/speech'\n",
    "speech_data = []\n",
    "\n",
    "for file in os.listdir(speech_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        parts = file.split('_')\n",
    "        if len(parts) == 3:\n",
    "            word = parts[1]\n",
    "            emotion = parts[2].replace('.wav', '')\n",
    "            speech_data.append({\n",
    "                'word': word,\n",
    "                'emotion': emotion,\n",
    "                'speech_path': os.path.join(speech_dir, file)\n",
    "            })\n",
    "\n",
    "speech_df = pd.DataFrame(speech_data)\n",
    "speech_df.to_csv('speech_word_dataset.csv', index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load Text Dataset\n",
    "# ----------------------------\n",
    "def load_csvs_from_dir(directory):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "text_train_df = load_csvs_from_dir(\"dataset/text/train\")\n",
    "text_val_df = load_csvs_from_dir(\"dataset/text/validation\")\n",
    "text_test_df = load_csvs_from_dir(\"dataset/text/test\")\n",
    "text_df = pd.concat([text_train_df, text_val_df, text_test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ee0ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Encode Labels (Shared)\n",
    "# ----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = pd.concat([speech_df['emotion'], text_df['label']], ignore_index=True)\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "speech_df['label'] = label_encoder.transform(speech_df['emotion'])\n",
    "text_df['label'] = label_encoder.transform(text_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77d0cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Tokenizer and BERT Model\n",
    "# ----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cefacf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Feature Extraction Utils\n",
    "# ----------------------------\n",
    "def extract_mfcc(wav_path, max_len=100):\n",
    "    y, sr = librosa.load(wav_path, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0,0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T\n",
    "def extract_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2bc938e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Early Fusion Dataset\n",
    "# ----------------------------\n",
    "class EarlyFusionDataset(Dataset):\n",
    "    def __init__(self, speech_df, text_df):\n",
    "        self.speech_df = speech_df.reset_index(drop=True)\n",
    "        self.text_df = text_df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.speech_df), len(self.text_df))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speech_row = self.speech_df.iloc[idx]\n",
    "        text_row = self.text_df.iloc[idx]\n",
    "\n",
    "        mfcc = extract_mfcc(speech_row['speech_path'])  # [time, 40]\n",
    "        label = torch.tensor(speech_row['label'], dtype=torch.long)\n",
    "\n",
    "        # Tokenize text for BERT\n",
    "        text_input = tokenizer(text_row['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        text_input = {k: v.squeeze(0) for k, v in text_input.items()}  # remove batch dim\n",
    "\n",
    "        return torch.tensor(mfcc, dtype=torch.float32), text_input, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8062f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7. Collate Function for Padding\n",
    "# ----------------------------\n",
    "def collate_fn(batch):\n",
    "    mfccs, text_inputs, labels = zip(*batch)\n",
    "\n",
    "    # Pad MFCC sequences\n",
    "    mfccs = nn.utils.rnn.pad_sequence(mfccs, batch_first=True)\n",
    "\n",
    "    # Stack BERT inputs\n",
    "    input_ids = torch.stack([ti['input_ids'] for ti in text_inputs])\n",
    "    attention_mask = torch.stack([ti['attention_mask'] for ti in text_inputs])\n",
    "    text_input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "    return mfccs, text_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf085793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8. BERT Model\n",
    "# ----------------------------\n",
    "class BERTFusionModel(nn.Module):\n",
    "    def __init__(self, bert_model, mfcc_dim=40, audio_pooling='avg', num_classes=6):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.audio_pooling = audio_pooling\n",
    "        \n",
    "        # If using average pooling for MFCC\n",
    "        self.audio_proj = nn.Linear(mfcc_dim, 128)  # compress MFCC features\n",
    "        \n",
    "        # Fusion Layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768 + 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, mfcc, text_input):\n",
    "        # Get BERT features\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert(**text_input)\n",
    "            text_feat = bert_output.last_hidden_state[:, 0, :]  # CLS token: [batch, 768]\n",
    "\n",
    "        # Aggregate MFCC (avg over time dimension)\n",
    "        audio_feat = mfcc.mean(dim=1)  # [batch, 40]\n",
    "        audio_feat = self.audio_proj(audio_feat)  # [batch, 128]\n",
    "\n",
    "        # Combine\n",
    "        fused = torch.cat((text_feat, audio_feat), dim=1)  # [batch, 896]\n",
    "        out = self.fusion(fused)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4b0ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3470, Train Acc: 22.92%, Val Loss: 1.3296, Val Acc: 63.75%\n",
      "Epoch 2, Train Loss: 1.3269, Train Acc: 49.27%, Val Loss: 0.9661, Val Acc: 95.42%\n",
      "Epoch 3, Train Loss: 0.9584, Train Acc: 70.62%, Val Loss: 0.7116, Val Acc: 96.67%\n",
      "Epoch 4, Train Loss: 0.6926, Train Acc: 82.40%, Val Loss: 0.5441, Val Acc: 95.83%\n",
      "Epoch 5, Train Loss: 0.5322, Train Acc: 90.42%, Val Loss: 0.3934, Val Acc: 98.75%\n",
      "Epoch 6, Train Loss: 0.3828, Train Acc: 94.06%, Val Loss: 0.3084, Val Acc: 98.75%\n",
      "Epoch 7, Train Loss: 0.3125, Train Acc: 95.52%, Val Loss: 0.2289, Val Acc: 98.75%\n",
      "Epoch 8, Train Loss: 0.2334, Train Acc: 97.81%, Val Loss: 0.1894, Val Acc: 97.50%\n",
      "Epoch 9, Train Loss: 0.1973, Train Acc: 97.40%, Val Loss: 0.1546, Val Acc: 99.58%\n",
      "Epoch 10, Train Loss: 0.1661, Train Acc: 98.33%, Val Loss: 0.1350, Val Acc: 99.17%\n",
      "Epoch 11, Train Loss: 0.1400, Train Acc: 98.33%, Val Loss: 0.1130, Val Acc: 99.58%\n",
      "Epoch 12, Train Loss: 0.1064, Train Acc: 99.38%, Val Loss: 0.0994, Val Acc: 99.58%\n",
      "Epoch 13, Train Loss: 0.0991, Train Acc: 98.96%, Val Loss: 0.0816, Val Acc: 99.58%\n",
      "Epoch 14, Train Loss: 0.0876, Train Acc: 99.17%, Val Loss: 0.0774, Val Acc: 99.58%\n",
      "Epoch 15, Train Loss: 0.0722, Train Acc: 99.79%, Val Loss: 0.0673, Val Acc: 99.58%\n",
      "Epoch 16, Train Loss: 0.0648, Train Acc: 99.58%, Val Loss: 0.0597, Val Acc: 99.58%\n",
      "Epoch 17, Train Loss: 0.0584, Train Acc: 99.58%, Val Loss: 0.0624, Val Acc: 99.58%\n",
      "Epoch 18, Train Loss: 0.0572, Train Acc: 99.58%, Val Loss: 0.0493, Val Acc: 99.17%\n",
      "Epoch 19, Train Loss: 0.0514, Train Acc: 99.48%, Val Loss: 0.0467, Val Acc: 99.58%\n",
      "Epoch 20, Train Loss: 0.0416, Train Acc: 99.79%, Val Loss: 0.0409, Val Acc: 99.58%\n",
      "Epoch 21, Train Loss: 0.0450, Train Acc: 99.38%, Val Loss: 0.0431, Val Acc: 99.58%\n",
      "Epoch 22, Train Loss: 0.0397, Train Acc: 99.58%, Val Loss: 0.0382, Val Acc: 99.58%\n",
      "Epoch 23, Train Loss: 0.0320, Train Acc: 99.90%, Val Loss: 0.0360, Val Acc: 99.58%\n",
      "Epoch 24, Train Loss: 0.0341, Train Acc: 99.90%, Val Loss: 0.0308, Val Acc: 99.58%\n",
      "Epoch 25, Train Loss: 0.0284, Train Acc: 99.79%, Val Loss: 0.0307, Val Acc: 99.58%\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 9. Training and Evaluation\n",
    "# ----------------------------\n",
    "\n",
    "# Split dataset into train and validation\n",
    "full_dataset = EarlyFusionDataset(speech_df, text_df)\n",
    "train_indices, val_indices = train_test_split(list(range(len(full_dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTFusionModel(bert_model=bert_model, num_classes=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mfccs, text_inputs, labels in loader:\n",
    "            mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "            text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "\n",
    "            outputs = model(mfccs, text_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Training loop with metrics\n",
    "for epoch in range(25):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for mfccs, text_inputs, labels in train_loader:\n",
    "        mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mfccs, text_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    train_acc = total_correct / total_samples\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    val_acc, val_loss = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_acc*100:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
